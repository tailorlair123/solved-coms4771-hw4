Download Link: https://assignmentchef.com/product/solved-coms4771-hw4
<br>
<ul>

 <li>[Estimating parameters with complete and incomplete data] Consider the data generation process for observation pair (<em>a,b</em>) as follows:

  <ul>

   <li><em>a </em>is the outcome of an independent six-faced (possibly loaded) dice-roll. That is, chance of rolling face ‘1’ is <em>p</em><sub>1</sub>, rolling face ‘2’ is <em>p</em><sub>2</sub>, etc., with a total of six distinct possibilities.</li>

   <li>Given the outcome <em>a</em>, <em>b </em>is drawn independently from a density distributed as <em>q<sub>a</sub>e</em><sup>−<em>q</em></sup><em><sup>a</sup></em><em><sup>b </sup></em>(where <em>q<sub>a </sub>&gt; </em>0).</li>

   <li>List all the parameters of this process. We shall denote the collection of all the parameters as the variable <em>θ </em>(the parameter vector).</li>

   <li>Suppose we run this process <em>n </em>times independently, and get the sequence:</li>

  </ul></li>

</ul>

(<em>a</em><sub>1</sub><em>,b</em><sub>1</sub>)<em>,</em>(<em>a</em><sub>2</sub><em>,b</em><sub>2</sub>)<em>,…,</em>(<em>a<sub>n</sub>,b<sub>n</sub></em>)<em>.</em>

What is the likelihood that this sequence was generated by a specific setting of the parameter vector <em>θ</em>?

<ul>

 <li>What is the most likely setting of the parameter vector <em>θ </em>given the complete observation sequence ? that is, find the Maximum Likelihood Estimate of <em>θ </em>given the observations.</li>

 <li>What is the probability of the partial (incomplete) observation <em>b</em><sub>1</sub><em>,b</em><sub>2</sub><em>,…,b<sub>n </sub></em>given a specific setting of the parameter vector <em>θ</em>?</li>

 <li>Derive the Expectation Maximization (EM) algirthm to estimate of the parameters given the incomplete observation sequence .</li>

</ul>

<ul>

 <li>[kernelizing <em>k</em>-means] <em>k</em>-means with Euclidean distance metric assumes that each pair of clusters is linearly separable. This may not be the case. A classic example is where we have two clusters corresponding to data points on two concentric circles in the R<sup>2</sup>.

  <ul>

   <li>Implement Lloyd’s method for <em>k</em>-means algorithm and show the resulting cluster assignment for the dataset depicted above. Give two more examples of datasets in R<sup>2</sup>, for which optimal <em>k</em>-means setting results in an undesirable clustering. Show the resulting cluster assignment for the two additional example datasets.</li>

  </ul></li>

</ul>

Let <em>φ </em>denote an explicit non-linear feature space mapping in some inner product space. We will show how one can derive an <em>implicit </em>kernel-based version of the Lloyd’s method for <em>k</em>-means, where we only operate on data as <em>φ</em>(<em>x<sub>i</sub></em>) · <em>φ</em>(<em>x<sub>j</sub></em>) = <em>K</em>(<em>x<sub>i</sub>,x<sub>j</sub></em>).

<ul>

 <li>Let <em>z<sub>ij </sub></em>be an indicator that is equal to 1 if the <em>φ</em>(<em>x<sub>i</sub></em>) is currently assigned to the <em>jth </em>cluster and 0 otherwise (1 ≤ <em>i </em>≤ <em>n </em>and 1 ≤ <em>j </em>≤ <em>k</em>). Show that the <em>jth </em>cluster center <em>c<sub>j </sub></em>can be written as, where <em>α<sub>ij </sub></em>only depends on <em>z<sub>ij </sub></em></li>

 <li>Given any two data points <em>φ</em>(<em>x<sub>i</sub></em>) and <em>φ</em>(<em>x<sub>j</sub></em>), show that the square distance k<em>φ</em>(<em>x<sub>i</sub></em>) − <em>φ</em>(<em>x<sub>j</sub></em>)k<sup>2 </sup>can be computed using only (linear combinations of) inner products.</li>

 <li>Given the results of parts (ii) and (iii), show how to compute the square distance k<em>φ</em>(<em>x<sub>i</sub></em>)− <em>c<sub>j</sub></em>k<sup>2 </sup>using only (linear combinations of) inner products between the data points <em>x</em><sub>1</sub><em>,…,x<sub>n</sub></em>.</li>

 <li>From results from parts (ii), (iii) and (iv), propose the algorithm for kernelized version of Lloyd’s method of finding <em>k</em>-means.</li>

 <li>Implement your proposed kernelized <em>k</em>-means method and run it on the three example datasets of part (i). Compare the resulting cluster for various choices of kernels (e.g.</li>

</ul>

linear kernel, quadratic kernel, rbf kernel).

(submit your datasets and kernelized code on Courseworks to receive full credit)

<ul>

 <li>[Randomized decision rules] Consider a “reward-based” prediction framework, where given a continuous state-space <em>X </em>and a discreet action-space <em>A</em>, a learning algorithm <em>f </em>: <em>X </em>→ <em>A </em>decides to perform action <em>a </em>∈ <em>A </em>when it is in state <em>x </em>∈ <em>X</em>. The learner then receives a numerical (possibly negative) reward <em>R</em>(<em>x,a</em>) for performing action <em>a </em>∈ <em>A </em>in state <em>x </em>∈ <em>X</em>. (Note: such reward-based frameworks are common in robotics where the learning agent, i.e., the robot, performs some—possibly randomized—action <em>a </em>in some situation <em>x </em>and receives reward <em>R</em>(<em>x,a</em>). The goal of the learning agent is to maximize its expected reward.)

  <ul>

   <li>Assume that <em>f </em>is allowed take a <em>randomized </em>action on any state <em>x</em>. That is, in any state <em>x </em>∈ <em>X</em>, <em>f </em>chooses to perform action <em>a </em>with probability <em>P</em>[<em>a</em>|<em>x</em>]. Show that the average (over the states) expected reward received by <em>f</em>, i.e., E<em>x,f</em><sub>(<em>x</em>)</sub>[<em>R</em>(<em>x,f</em>(<em>x</em>))] is</li>

   <li>Show that the expected reward is maximized by choosing <em>P</em>[<em>a</em>|<em>x</em>] = 1 for the action <em>a </em>associated with the maximal reward <em>R</em>(<em>x,a</em>) (for a given state <em>x</em>). This shows us that there is <em>no benefit </em>in randomizing the best decision rule.</li>

   <li>Can one benefit from randomizing a suboptimal rule? Briefly explain.</li>

  </ul></li>

 <li>[From distances to embeddings] Your friend from overseas is visiting you and asks you the geographical locations of popular US cities on a map. Not having access to a US map, you realize that you cannot provide your friend accurate information. You recall that you have access to the relative distances between nine popular US cities, given by the following distance matrix <em>D</em>:</li>

</ul>

<table width="476">

 <tbody>

  <tr>

   <td width="91">Distances (<em>D</em>)</td>

   <td width="50">BOS</td>

   <td width="44">NYC</td>

   <td width="43">DC</td>

   <td width="43">MIA</td>

   <td width="43">CHI</td>

   <td width="43">SEA</td>

   <td width="43">SF</td>

   <td width="43">LA</td>

   <td width="35">DEN</td>

  </tr>

  <tr>

   <td width="91">BOS</td>

   <td width="50">0</td>

   <td width="44">206</td>

   <td width="43">429</td>

   <td width="43">1504</td>

   <td width="43">963</td>

   <td width="43">2976</td>

   <td width="43">3095</td>

   <td width="43">2979</td>

   <td width="35">1949</td>

  </tr>

  <tr>

   <td width="91">NYC</td>

   <td width="50">206</td>

   <td width="44">0</td>

   <td width="43">233</td>

   <td width="43">1308</td>

   <td width="43">802</td>

   <td width="43">2815</td>

   <td width="43">2934</td>

   <td width="43">2786</td>

   <td width="35">1771</td>

  </tr>

  <tr>

   <td width="91">DC</td>

   <td width="50">429</td>

   <td width="44">233</td>

   <td width="43">0</td>

   <td width="43">1075</td>

   <td width="43">671</td>

   <td width="43">2684</td>

   <td width="43">2799</td>

   <td width="43">2631</td>

   <td width="35">1616</td>

  </tr>

  <tr>

   <td width="91">MIA</td>

   <td width="50">1504</td>

   <td width="44">1308</td>

   <td width="43">1075</td>

   <td width="43">0</td>

   <td width="43">1329</td>

   <td width="43">3273</td>

   <td width="43">3053</td>

   <td width="43">2687</td>

   <td width="35">2037</td>

  </tr>

  <tr>

   <td width="91">CHI</td>

   <td width="50">963</td>

   <td width="44">802</td>

   <td width="43">671</td>

   <td width="43">1329</td>

   <td width="43">0</td>

   <td width="43">2013</td>

   <td width="43">2142</td>

   <td width="43">2054</td>

   <td width="35">996</td>

  </tr>

  <tr>

   <td width="91">SEA</td>

   <td width="50">2976</td>

   <td width="44">2815</td>

   <td width="43">2684</td>

   <td width="43">3273</td>

   <td width="43">2013</td>

   <td width="43">0</td>

   <td width="43">808</td>

   <td width="43">1131</td>

   <td width="35">1307</td>

  </tr>

  <tr>

   <td width="91">SF</td>

   <td width="50">3095</td>

   <td width="44">2934</td>

   <td width="43">2799</td>

   <td width="43">3053</td>

   <td width="43">2142</td>

   <td width="43">808</td>

   <td width="43">0</td>

   <td width="43">379</td>

   <td width="35">1235</td>

  </tr>

  <tr>

   <td width="91">LA</td>

   <td width="50">2979</td>

   <td width="44">2786</td>

   <td width="43">2631</td>

   <td width="43">2687</td>

   <td width="43">2054</td>

   <td width="43">1131</td>

   <td width="43">379</td>

   <td width="43">0</td>

   <td width="35">1059</td>

  </tr>

  <tr>

   <td width="91">DEN</td>

   <td width="50">1949</td>

   <td width="44">1771</td>

   <td width="43">1616</td>

   <td width="43">2037</td>

   <td width="43">996</td>

   <td width="43">1307</td>

   <td width="43">1235</td>

   <td width="43">1059</td>

   <td width="35">0</td>

  </tr>

 </tbody>

</table>

Being a machine learning student, you believe that it may be possible to infer the locations of these cities from the distance data. To find an embedding of these nine cities on a two dimensional map, you decide to solve it as an optimization problem as follows.

You associate a two-dimensional variable <em>x<sub>i </sub></em>as the unknown latitude and the longitude value for each of the nine cities (that is, <em>x</em><sub>1 </sub>is the lat/lon value for BOS, <em>x</em><sub>2 </sub>is the lat/lon value for NYC, etc.). You write down the an (unconstrained) optimization problem

minimize<em>,</em>

<em>i,j</em>

where <sup>P</sup><em><sub>i,j</sub></em>(k<em>x<sub>i </sub></em>− <em>x<sub>j</sub></em>k − <em>D<sub>ij</sub></em>)<sup>2 </sup>denotes the embedding discrepancy function.

<ul>

 <li>What is the derivative of the discrepancy function with respect to a location <em>x<sub>i</sub></em>?</li>

 <li>Write a program in your preferred language to find an optimal setting of locations <em>x</em><sub>1</sub><em>,…,x</em><sub>9</sub>. You must submit your code to Courseworks to receive full credit.</li>

 <li>Plot the result of the optimization showing the estimated locations of the nine cities.</li>

</ul>

(here is a sample code to plot the city locations in Matlab)

&gt;&gt; cities={’BOS’,’NYC’,’DC’,’MIA’,’CHI’,’SEA’,’SF’,’LA’,’DEN’};

&gt;&gt; locs = [x1;x2;x3;x4;x5;x6;x7;x8;x9];

&gt;&gt; figure; text(locs(:,1), locs(:,2), cities);

What can you say about your result of the estimated locations compared to the actual geographical locations of these cities?